{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#To reset seed so same set of numbers are produced\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('banknotes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.89569</td>\n",
       "      <td>3.00250</td>\n",
       "      <td>-3.606700</td>\n",
       "      <td>-3.44570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.47690</td>\n",
       "      <td>-0.15314</td>\n",
       "      <td>2.530000</td>\n",
       "      <td>2.44950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.91020</td>\n",
       "      <td>6.06500</td>\n",
       "      <td>-2.453400</td>\n",
       "      <td>-0.68234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.60731</td>\n",
       "      <td>3.95440</td>\n",
       "      <td>-4.772000</td>\n",
       "      <td>-4.48530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.37180</td>\n",
       "      <td>7.49080</td>\n",
       "      <td>0.015989</td>\n",
       "      <td>-1.74140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0  -0.89569   3.00250 -3.606700 -3.44570      1\n",
       "1   3.47690  -0.15314  2.530000  2.44950      0\n",
       "2   3.91020   6.06500 -2.453400 -0.68234      0\n",
       "3   0.60731   3.95440 -4.772000 -4.48530      1\n",
       "4   2.37180   7.49080  0.015989 -1.74140      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputing the dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1372 entries, 0 to 1371\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   variance  1372 non-null   float64\n",
      " 1   skewness  1372 non-null   float64\n",
      " 2   curtosis  1372 non-null   float64\n",
      " 3   entropy   1372 non-null   float64\n",
      " 4   class     1372 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 53.7 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of null values column wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance    0\n",
       "skewness    0\n",
       "curtosis    0\n",
       "entropy     0\n",
       "class       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect unique values in every feature\n",
    "for col in data.columns:\n",
    "    print(\"{}:{}\".format(col,data[col].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic statistics feautre wise\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "#returns all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing values in the dataset\n",
    "\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "#activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(data=data, hue='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Design of a Neural Network from scratch\n",
    "\n",
    "# *************<IMP>*************\n",
    "# Mention hyperparameters used and describe functionality in detail in this space\n",
    "# - carries 1 mark\n",
    "# '''\n",
    "# class NN:\n",
    "    \n",
    "#     ''' X and Y are dataframes'''\n",
    "#     def __init__(self,numHlayers=2):\n",
    "        \n",
    "#         #number of hidden layers the ANN has\n",
    "#         self.numHlayers = numHlayers\n",
    "    \n",
    "#     def fit(self,X,Y):\n",
    "#         '''\n",
    "#         Function that trains the neural network by talking x_train and y_train samples as input\n",
    "#         '''\n",
    "#         input1=X\n",
    "#         result1=Y\n",
    "        \n",
    "#         #If the ANN has two hidden layer we initialize it calling ANN_layer class\n",
    "        \n",
    "#         if(self.numHlayers==2):\n",
    "            \n",
    "#             #First layer is initialized with parameter 1 :Number of inputs\n",
    "#             #                                parameter 2 :Number of neurons\n",
    "#             #similarly hidden layer and output layer is initialised\n",
    "#             self.Hlayer1 =ANN_layers(7,24)\n",
    "#             self.Hlayer2 =ANN_layers(24,24)\n",
    "#             self.Output_layer =ANN_layers(24,2)\n",
    "            \n",
    "#             #Between each layer we have the activation function class with a \n",
    "#             #wide range of activation functions \n",
    "            \n",
    "#             self.act1=activation()\n",
    "#             self.act2=activation()\n",
    "            \n",
    "#             # using the softmax function on the final layer \n",
    "#             self.loss_activation=Activation_softmax_LCCE()\n",
    "            \n",
    "#             #So for optimizer we can choose between Adam and SGD optimizer\n",
    "#             #Creating optimizer\n",
    "#             self.optimize=optimizer(l_rate=0.05,decay=5e-7)\n",
    "            \n",
    "            \n",
    "#             #Loop to train in\n",
    "#             for epoch in range(4001):\n",
    "                \n",
    "#                 #pass input X through first layer\n",
    "#                 self.Hlayer1.fpropogation(input1)\n",
    "                \n",
    "#                 #Output of first layer passes through the firstactivation function\n",
    "#                 self.act1.fRelu(self.Hlayer1.output)\n",
    "                \n",
    "#                 #Output of first activation is sent in forward pass to second layer\n",
    "#                 self.Hlayer2.fpropogation(self.act1.output)\n",
    "                \n",
    "#                  #Output ofsecond layer passes through the next activation function\n",
    "#                 self.act2.fRelu(self.Hlayer2.output)\n",
    "                \n",
    "#                 # Perform a forward pass through the activation/loss function\n",
    "#                 # takes the output of second dense layer here and returns loss\n",
    "#                 self.Output_layer.fpropogation(self.act2.output)\n",
    "#                 loss = self.loss_activation.forward(self.Output_layer.output,result1)\n",
    "                \n",
    "#                 #Argmax returns the predictions for the foward pass\n",
    "#                 prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "                \n",
    "#                 #calculate values along first axis\n",
    "#                 if len(result1.shape)==2:\n",
    "#                     result1=np.argmax(result1,axis=1)\n",
    "#                 #returns the accuracy of our predictions in this pass\n",
    "#                 accuracy= np.mean(prediction==result1)\n",
    "                \n",
    "#                 #priniting for every 100Th epoch \n",
    "#                 if not epoch %100:\n",
    "#                     print(f'epoch:{epoch} ,'+\n",
    "#                           f'accuracy:{accuracy:.3f} ,'+\n",
    "#                           f'loss:{loss} ,'+\n",
    "#                           f'lr:{self.optimize.curr_l_rate},')\n",
    "                    \n",
    "#                 #back propogation \n",
    "#                 #The values are propogated backwards and the dinputs are updated\n",
    "#                 self.loss_activation.backpass(self.loss_activation.output,result1)\n",
    "#                 self.Output_layer.Bpropogation(self.loss_activation.dinputs)\n",
    "#                 self.act2.back(self.Output_layer.dinputs)\n",
    "#                 self.Hlayer2.Bpropogation(self.act2.dinputs)\n",
    "#                 self.act1.back(self.Hlayer2.dinputs)\n",
    "#                 self.Hlayer1.Bpropogation(self.act1.dinputs)\n",
    "                \n",
    "                \n",
    "#                 #Update in weights and the biases\n",
    "#                 self.optimize.initial_update_param()\n",
    "#                 self.optimize.update_params(self.Hlayer1)\n",
    "#                 self.optimize.update_params(self.Hlayer2)\n",
    "#                 self.optimize.update_params(self.Output_layer)\n",
    "#                 self.optimize.post_update_params()\n",
    "        \n",
    "        \n",
    "#         #If ANN has 1 hidden layer we initialize it calling ANN_layer()\n",
    "        \n",
    "#         if(self.numHlayers==1):\n",
    "            \n",
    "#             # layers are initialized with parameter 1 :Number of inputs\n",
    "#             #                             parameter 2 :Number of neurons\n",
    "#             #similarly hidden layer and output layer is initialised\n",
    "            \n",
    "#             self.Hlayer1 =ANN_layers(7,128)\n",
    "#             self.Output_layer =ANN_layers(128,2)\n",
    "#             #Between each layer we have the activation function class with a \n",
    "#             #wide range of activation functions \n",
    "            \n",
    "#             self.act1=activation()\n",
    "            \n",
    "#             # using the softmax function on the final layer\n",
    "#             self.loss_activation=Activation_softmax_LCCE()\n",
    "            \n",
    "#             #So for optimizer we can choose between Adam and SGD optimizer\n",
    "#             #Creating optimizer\n",
    "#             self.optimize=optimizer(l_rate=0.05,decay=5e-7)\n",
    "            \n",
    "#             #Loop to train in\n",
    "#             for epoch in range(4001):\n",
    "                \n",
    "                \n",
    "#                 #pass input X through first layer\n",
    "#                 #Output of first layer passes through the firstactivation function\n",
    "#                 # Perform a forward pass through the activation/loss function\n",
    "#                 # takes the output of second dense layer here and returns loss\n",
    "#                 self.Hlayer1.fpropogation(input1)\n",
    "#                 self.act1.fRelu(self.Hlayer1.output)\n",
    "#                 self.Output_layer.fpropogation(self.act1.output)\n",
    "#                 loss = self.loss_activation.forward(self.Output_layer.output,result1)\n",
    "                \n",
    "#                 #Argmax returns the predictions for the foward pass\n",
    "#                 prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "                \n",
    "#                 #calculate values along first axis\n",
    "#                 if len(result1.shape)==2:\n",
    "#                     result1=np.argmax(result1,axis=1)\n",
    "               \n",
    "#                 #returns the accuracy of our predictions in this pass\n",
    "#                 accuracy= np.mean(prediction==result1)\n",
    "               \n",
    "                \n",
    "#                 #priniting for every 100Th epoch\n",
    "#                 if not epoch %100:\n",
    "#                     print(f'epoch:{epoch} ,'+\n",
    "#                           f'accuracy:{accuracy:.3f} ,'+\n",
    "#                           f'loss:{loss} ,'+\n",
    "#                           f'lr:{self.optimize.curr_l_rate},')\n",
    "                \n",
    "                \n",
    "#                 #back propogation \n",
    "#                 #The values are propogated backwards and the dinputs are updated\n",
    "                \n",
    "#                 self.loss_activation.backpass(self.loss_activation.output,result1)\n",
    "#                 self.Output_layer.Bpropogation(self.loss_activation.dinputs)\n",
    "#                 self.act1.back(self.Output_layer.dinputs)\n",
    "#                 self.Hlayer1.Bpropogation(self.act1.dinputs)\n",
    "                \n",
    "                \n",
    "#                 #Update in weights and the biases\n",
    "\n",
    "#                 self.optimize.initial_update_param()\n",
    "#                 self.optimize.update_params(self.Hlayer1)\n",
    "#                 self.optimize.update_params(self.Output_layer)\n",
    "#                 self.optimize.post_update_params()\n",
    "                \n",
    "               \n",
    "    \n",
    "    \n",
    "#     def predict(self,X,Y):\n",
    "\n",
    "#         \"\"\"\n",
    "#         The predict function performs a simple feed forward of weights\n",
    "#         and outputs yhat values \n",
    "\n",
    "#         yhat is a list of the predicted value for df X\n",
    "        \n",
    "#         Only Forward pass to be performed in already trained model\n",
    "#         \"\"\"\n",
    "        \n",
    "#         #Only Forward pass to be performed in already trained model\n",
    "#         x_test=X\n",
    "#         y_test=Y\n",
    "#         #If ANN has 2 hidden layer we call the  ANN_layer() objects\n",
    "#         if(self.numHlayers==2):\n",
    "#             #pass input to the first layer \n",
    "#             self.Hlayer1.fpropogation(x_test)\n",
    "            \n",
    "#             #layer 1 activation func\n",
    "#             self.act1.fRelu(self.Hlayer1.output)\n",
    "            \n",
    "#             #output of act1 to 2nd layer\n",
    "#             self.Hlayer2.fpropogation(self.act1.output)\n",
    "            \n",
    "#             #layer 2 activation func\n",
    "#             self.act2.fRelu(self.Hlayer2.output)\n",
    "#             self.Output_layer.fpropogation(self.act2.output)\n",
    "            \n",
    "#             #loss/activation for output layer\n",
    "#             loss = self.loss_activation.forward(self.Output_layer.output,y_test)\n",
    "            \n",
    "#             #Argmax returns our models predictions\n",
    "#             prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "#             if len(y_test.shape)==2:\n",
    "#                 result1=np.argmax(y_test,axis=1)\n",
    "            \n",
    "#             #Accuracy out model has attained\n",
    "#             accuracy= np.mean(prediction==y_test)\n",
    "#             print(f'accuracy:{accuracy:.3f} ,'+\n",
    "#                   f'lr:{self.optimize.curr_l_rate},')\n",
    "            \n",
    "#             #return the prediction\n",
    "#             yhat=prediction\n",
    "#             return yhat\n",
    "       \n",
    "    \n",
    "#         #If ANN has 1 hidden layer we call the  ANN_layer() objects\n",
    "#         if(self.numHlayers==1):\n",
    "#             #pass input to the first layer \n",
    "#             self.Hlayer1.fpropogation(x_test)\n",
    "            \n",
    "#             #layer 1 activation func\n",
    "#             self.act1.fRelu(self.Hlayer1.output)\n",
    "            \n",
    "#             self.Output_layer.fpropogation(self.act1.output)\n",
    "#             #loss/activation for output layer\n",
    "#             loss = self.loss_activation.forward(self.Output_layer.output,y_test)\n",
    "            \n",
    "#             #Argmax returns our models predictions\n",
    "#             prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "#             if len(y_test.shape)==2:\n",
    "#                 result1=np.argmax(y_test,axis=1)\n",
    "            \n",
    "#             #Accuracy out model has attained\n",
    "#             accuracy= np.mean(prediction==y_test)\n",
    "#             print(f'accuracy:{accuracy:.3f} ,'+\n",
    "#                   f'lr:{self.optimize.curr_l_rate},')\n",
    "#             #return the prediction\n",
    "#             yhat=prediction\n",
    "#             return yhat\n",
    "        \n",
    "    \n",
    "#     def CM(self,y_test,y_test_obs):\n",
    "        \n",
    "#         \"\"\"\n",
    "#         Since our prediction are already given in list np.array we dont \n",
    "#         need to convert confidence values\n",
    "#         \"\"\"\n",
    "#         #2x2 matrix \n",
    "#         cm=np.array([[0,0],[0,0]])\n",
    "#         fp=0 #false positive\n",
    "#         fn=0 #false negative\n",
    "#         tp=0 #true Positive\n",
    "#         tn=0 #true negative\n",
    "        \n",
    "#         #loop over number of samples\n",
    "#         for i in range(len(y_test)):\n",
    "#             if(y_test[i]==1 and y_test_obs[i]==1):\n",
    "#                 tp=tp+1\n",
    "#             if(y_test[i]==0 and y_test_obs[i]==0):\n",
    "#                 tn=tn+1\n",
    "#             if(y_test[i]==1 and y_test_obs[i]==0):\n",
    "#                 fp=fp+1\n",
    "#             if(y_test[i]==0 and y_test_obs[i]==1):\n",
    "#                 fn=fn+1\n",
    "#         cm[0][0]=tn\n",
    "#         cm[0][1]=fp\n",
    "#         cm[1][0]=fn\n",
    "#         cm[1][1]=tp\n",
    "        \n",
    "#         p= tp/(tp+fp) #precision\n",
    "#         r=tp/(tp+fn)  #recall\n",
    "#         f1=(2*p*r)/(p+r) #f1 score\n",
    "        \n",
    "        \n",
    "#         print(\"Confusion Matrix : \")\n",
    "#         print(cm)\n",
    "#         print(\"\\n\")\n",
    "#         print(f\"Precision : {p}\")\n",
    "#         print(f\"Recall : {r}\")\n",
    "#         print(f\"F1 SCORE : {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer class so each layer can be intilialised as an object \n",
    "class ANN_layers:\n",
    "    \n",
    "    # layer properties\n",
    "    def __init__(self,num_inputs ,num_cells):\n",
    "        \n",
    "        #initializing weights and biases\n",
    "        self.weights = 0.1 * np.random.randn(num_inputs,num_cells) #in range[-1,1]\n",
    "        self.biases = np.zeros((1,num_cells))\n",
    "    \n",
    "    #forward pass\n",
    "    def fpropogation(self , inputs):\n",
    "        \n",
    "        #forward pass\n",
    "        self.inputs= inputs\n",
    "        \n",
    "        #Calculate output values from inputs,weights and biases\n",
    "        self.output= np.dot(inputs ,self.weights)\n",
    "    \n",
    "    #back pass\n",
    "    def Bpropogation(self,dval):\n",
    "        #gradient on parameters and values\n",
    "        self.dweights= np.dot(self.inputs.T,dval)\n",
    "        self.dbiases = np.sum(dval,axis=0,keepdims=True)\n",
    "        self.dinputs =np.dot(dval,self.weights.T)\n",
    "\n",
    "def confidenceTolist(x):\n",
    "    #to convert a given confidence matrix to list of predictions\n",
    "    rows = x.shape[0]\n",
    "    cols = x.shape[1]\n",
    "    predict=[None]*rows\n",
    "    for i in range(0,rows):\n",
    "        if(x[i,0]>x[i,1]):\n",
    "            predict[i]=0\n",
    "        else:\n",
    "            predict[i]=1\n",
    "    return predict\n",
    "        \n",
    "        \n",
    "    \n",
    "#Activation class with different activation func        \n",
    "class activation:\n",
    "    \n",
    "    #RELU activation\n",
    "    #makes copy of input values\n",
    "    def fRelu(self,x):\n",
    "        self.inputs=x\n",
    "        self.output = np.maximum(0,x)\n",
    "        #sigmoid\n",
    "        #self.inputs =x\n",
    "        #self.output = 1/(1+np.exp(-x))\n",
    "        #self.output = np.tanh(x) #tanh\n",
    "    \n",
    "    #sigmoid activation\n",
    "    def sigmoid(self,x):\n",
    "        self.inputs =x\n",
    "        self.output = 1/(1+np.exp(-x))\n",
    "    \n",
    "    #tanh activation\n",
    "    def tanh(self,x):\n",
    "        self.inputs =x\n",
    "        self.output = np.tanh(x)\n",
    "        \n",
    "    #for back pass\n",
    "    def back(self,dval):\n",
    "        #copy of back pass inputs\n",
    "        self.dinputs = dval.copy()\n",
    "        \n",
    "        #Gradient to be set ) for -ve values\n",
    "        self.dinputs[self.inputs <=0] =0 \n",
    "        \n",
    "#Softmax activation\n",
    "class act_softmax :       \n",
    "    \n",
    "    #forward pass\n",
    "    def fsoftmax(self,inputs):\n",
    "        \n",
    "        #make copy of input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #unnormalised probabities\n",
    "        exp1 = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        #normalising\n",
    "        prob = exp1 /np.sum(exp1 ,axis=1, keepdims=True)\n",
    "        \n",
    "         #storing and calculating output\n",
    "        self.output = prob\n",
    "    \n",
    "    #Back pass\n",
    "    def bsoftmax(self,dval):\n",
    "        \n",
    "        #create empty array\n",
    "        self.dinputs = np.empty_like(dval)\n",
    "        \n",
    "        #enumerate output and gradients\n",
    "        for i ,(single_o,single_dval) in enumerate(zip(self.output,dval)):\n",
    "            \n",
    "            #reshape output array\n",
    "            single_o = single_o.reshape(-1,1)\n",
    "            \n",
    "            #Calculate the jacobian matrix of the output\n",
    "            jacobian = np.diagflat(single_o)-np.dot(single_o,single_o.T)\n",
    "            \n",
    "            self.dinputs[i] = np.dot(jacobian,single_dval)\n",
    "\n",
    "#Loss class        \n",
    "class loss:\n",
    "    \n",
    "    #for given model output and ground truth values calculate loss\n",
    "    def cal(self,output,y):\n",
    "        \n",
    "        sampleLosses = self.check(output,y)\n",
    "        \n",
    "        #average loss\n",
    "        data_loss= np.mean(sampleLosses)\n",
    "        \n",
    "        #return the loss\n",
    "        return data_loss\n",
    "\n",
    "#Cross Entropy Loss  inheriting the loss class    \n",
    "class CCE(loss):\n",
    "    #for forward pass\n",
    "    def check(self,y_pred,y_true):\n",
    "        \n",
    "        #number of samples\n",
    "        x = len(y_pred)\n",
    "        \n",
    "        #clip data to prevent divison by 0\n",
    "        y_pred_clipped = np.clip(y_pred ,1e-7,1 - 1e-7)\n",
    "        \n",
    "        #probablities for target values\n",
    "        confidence = y_pred_clipped[range(x),y_true]\n",
    "        negLikely = -np.log(confidence)\n",
    "        \n",
    "        #Return losses\n",
    "        return negLikely\n",
    "    \n",
    "    #for back pass\n",
    "    def backward(self,dval,y_true):\n",
    "        \n",
    "        #number of samples\n",
    "        x=len(dval)\n",
    "        \n",
    "        # Number of labels in every sample\n",
    "        # use the first sample to count them\n",
    "        \n",
    "        label=len(dval[0])\n",
    "        \n",
    "        if len(y_true.shape)==1:\n",
    "            y_true=np.eye(label)[y_true] #\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs =-y_true/dval\n",
    "        # Normalize gradient\n",
    "        self.dinputs =self.dinputs/x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer: #SGD\n",
    "    \n",
    "    #initialize optimizer \n",
    "    #set default settings\n",
    "    def __init__(self , l_rate=1.,decay=0.,p=0.):\n",
    "        self.l_rate = l_rate\n",
    "        self.curr_l_rate=l_rate\n",
    "        self.decay = decay\n",
    "        self.iter =0\n",
    "        self.p =p\n",
    "        \n",
    "    #Call before any parameters are updated\n",
    "    #After every back pass\n",
    "    def initial_update_param(self):\n",
    "        if self.decay:\n",
    "            self.curr_l_rate = self.l_rate *(1./(1+self.decay*self.iter))\n",
    "            \n",
    "    #Update the parameters\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        #if momentum is used\n",
    "        if self.p:\n",
    "            \n",
    "            #if the layer doesnt have momentum atrribute create them filled with 0\n",
    "            \n",
    "            if not hasattr(layer,'wieght_momentums'):\n",
    "                layer.weight_p = np.zeros_like(layer.weights)\n",
    "                layer.bias_p = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            \n",
    "            \n",
    "            \n",
    "            w_updates= self.p*layer.weight_p - self_curr_l_rate*layer.dweights\n",
    "            layer.weight_p = w_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            b_updates= self.p*layer.bias_p - self_curr_l_rate*layer.dbiases\n",
    "            layer.bias_p = b_updates\n",
    "        \n",
    "        # SGD updates (as before momentum update)\n",
    "        else:\n",
    "            w_updates =-self.curr_l_rate * layer.dweights\n",
    "            b_updates =-self.curr_l_rate * layer.dbiases\n",
    "        \n",
    "        # Update weights and biases using either\n",
    "        #  momentum updates\n",
    "        \n",
    "        \n",
    "        layer.weights+=w_updates\n",
    "        layer.biases+=b_updates\n",
    "    \n",
    "    \n",
    "    #update iterations after every update    \n",
    "    def post_update_params(self):\n",
    "        self.iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam optimizer        \n",
    "class  Adam_op:\n",
    "    \n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self , l_rate=.001,decay=0.,ep=1e-7,b1=0.9,b2=0.999):\n",
    "        self.l_rate = l_rate\n",
    "        self.curr_l_rate=l_rate\n",
    "        self.decay = decay\n",
    "        self.iter =0\n",
    "        self.ep =ep\n",
    "        self.b1=b1\n",
    "        self.b2=b2\n",
    "    \n",
    "    #Call before any parameters are updated\n",
    "    #After every back pass\n",
    "    def initial_update_param(self):\n",
    "        if self.decay:\n",
    "            self.curr_l_rate = self.l_rate *(1./(1+self.decay*self.iter))\n",
    "            \n",
    "    #Update the parameters\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        #If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer,'w_cache'):\n",
    "            layer.w_p =np.zeros_like(layer.weights)\n",
    "            layer.w_cache=np.zeros_like(layer.weights)\n",
    "            layer.bias_p=np.zeros_like(layer.biases)\n",
    "            layer.bias_cache=np.zeros_like(layer.biases)\n",
    "            \n",
    "        #Update momentum with current gradients\n",
    "        layer.w_p = self.b1*layer.w_p + (1-self.b1)*layer.dweights\n",
    "        layer.bias_p=self.b1*layer.bias_p + (1-self.b1)*layer.dbiases\n",
    "        \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        w_p_corrected = layer.w_p/(1-self.b1**(self.iter +1))\n",
    "        bias_p_corrected = layer.bias_p/(1-self.b1**(self.iter +1))\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.w_cache = self.b2 * layer.w_cache + (1-self.b2)*layer.dweights**2\n",
    "        layer.bias_cache = self.b2*layer.bias_cache+(1-self.b2)*layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        w_cache_corrected = layer.w_cache/(1-self.b2)**(self.iter+1)\n",
    "        bias_cache_corrected = layer.bias_cache/(1-self.b2)**(self.iter+1)\n",
    "        \n",
    "        \n",
    "        # SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        \n",
    "        layer.weights+=-self.curr_l_rate*w_p_corrected/(np.sqrt(w_cache_corrected)+self.ep)\n",
    "        layer.biases+=-self.curr_l_rate*bias_p_corrected/(np.sqrt(bias_cache_corrected)+self.ep)\n",
    "    \n",
    "    #update iterations after every update    \n",
    "    def post_update_params(self):\n",
    "        self.iter+=1\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_softmax_LCCE():\n",
    "    \n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation=act_softmax()\n",
    "        self.loss=CCE()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self,inputs,y_true):\n",
    "        \n",
    "        # Output layer's activation function\n",
    "        self.activation.fsoftmax(inputs)\n",
    "        \n",
    "        #store the output\n",
    "        self.output=self.activation.output\n",
    "        \n",
    "        #Calculate and return loss value\n",
    "        return self.loss.cal(self.output,y_true)\n",
    "        \n",
    "    # Backward pass\n",
    "    def backpass(self,dval,y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        x=len(dval)\n",
    "        \n",
    "        \n",
    "        if len(y_true.shape)==2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        \n",
    "        \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs =dval.copy()\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(x),y_true]-=1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs= self.dinputs/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2=data.loc[:,['variance', 'skewness', 'curtosis', 'entropy']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing test train split\n",
    "input1,x_test, result1, y_test = train_test_split(input2, result, test_size=0.35, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating first model\n",
    "#If the ANN has two hidden layer we initialize it calling ANN_layer class\n",
    "#First layer is initialized with parameter 1 :Number of inputs\n",
    "            #                   parameter 2 :Number of neurons\n",
    "            #similarly hidden layer and output layer is initialised\n",
    "Hlayer1=ANN_layers(4,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hlayer2=ANN_layers(16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_layer=ANN_layers(16,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Between each layer we have the activation function class with a \n",
    "#wide range of activation functions \n",
    "act1=activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act2=activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the softmax function on the final layer \n",
    "loss_activation=Activation_softmax_LCCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So for optimizer we can choose between Adam and SGD optimizer\n",
    "#Creating optimizer\n",
    "optimize = optimizer(l_rate=0.025,decay=5e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loop to train in\n",
    "for epoch in range(4001):\n",
    "    \n",
    "    #pass input X through first layer\n",
    "    Hlayer1.fpropogation(input1)\n",
    "    \n",
    "    #Output of first layer passes through the firstactivation function\n",
    "    act1.fRelu(Hlayer1.output)\n",
    "    \n",
    "    #Output of first activation is sent in forward pass to second layer\n",
    "    Hlayer2.fpropogation(act1.output)\n",
    "    \n",
    "    #Output ofsecond layer passes through the next activation function\n",
    "    act2.fRelu(Hlayer2.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    Output_layer.fpropogation(act2.output)\n",
    "    loss = loss_activation.forward(Output_layer.output,result1)\n",
    "    \n",
    "    #Argmax returns the predictions for the foward pass\n",
    "    prediction = np.argmax(loss_activation.output,axis=1)\n",
    "    \n",
    "    #calculate values along first axis\n",
    "    if len(result1.shape)==2:\n",
    "        result1=np.argmax(result1,axis=1)\n",
    "    \n",
    "    \n",
    "    accuracy= np.mean(prediction==result1)\n",
    "    #returns the accuracy of our predictions in this pass\n",
    "    if not epoch %100:\n",
    "        print(f'epoch:{epoch} ,'+\n",
    "              f'accuracy:{accuracy:.3f} ,'+\n",
    "              f'loss:{loss} ,'+\n",
    "              f'lr:{optimize.curr_l_rate},')\n",
    "    \n",
    "    #back propogation \n",
    "    #The values are propogated backwards and the dinputs are updated\n",
    "    loss_activation.backpass(loss_activation.output,result1)\n",
    "    Output_layer.Bpropogation(loss_activation.dinputs)\n",
    "    act2.back(Output_layer.dinputs)\n",
    "    Hlayer2.Bpropogation(act2.dinputs)\n",
    "    act1.back(Hlayer2.dinputs)\n",
    "    Hlayer1.Bpropogation(act1.dinputs)\n",
    "              \n",
    "    #Update in weights and the biases\n",
    "    optimize.initial_update_param()\n",
    "    optimize.update_params(Hlayer1)\n",
    "    optimize.update_params(Hlayer2)\n",
    "    optimize.update_params(Output_layer)\n",
    "    optimize.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only Forward pass to be performed in already trained model\n",
    "\n",
    "#pass input to the first layer\n",
    "Hlayer1.fpropogation(x_test)\n",
    "\n",
    "#layer 1 activation func\n",
    "act1.fRelu(Hlayer1.output)\n",
    "\n",
    "#output of act1 to 2nd layer\n",
    "Hlayer2.fpropogation(act1.output)\n",
    "\n",
    "#layer 2 activation func\n",
    "act2.fRelu(Hlayer2.output)\n",
    "\n",
    "\n",
    "Output_layer.fpropogation(act2.output)\n",
    "\n",
    "#loss/activation for output layer\n",
    "loss = loss_activation.forward(Output_layer.output,y_test)\n",
    "\n",
    "#Argmax returns our models predictions\n",
    "prediction = np.argmax(loss_activation.output,axis=1)\n",
    "if len(y_test.shape)==2:\n",
    "    result1=np.argmax(y_test,axis=1)\n",
    "\n",
    "#Accuracy out model has attained    \n",
    "accuracy= np.mean(prediction==y_test)\n",
    "print(f'epoch:{epoch} ,'+f'accuracy:{accuracy:.3f} ,'+\n",
    "      f'lr:{optimize.curr_l_rate},')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=NN() #to call confusion matrix func\n",
    "model1.CM(y_test.tolist(),prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since dataset is biased towards 1 we will try SMOTE approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data[(data['class']==0)]\n",
    "data2.head(3)\n",
    "new_data=pd.concat([data,data2])\n",
    "#new_data=pd.concat([new_data,data2])\n",
    "new_data.head(3)\n",
    "new_data=new_data.sample(frac=1)\n",
    "new_data.head(3)\n",
    "new_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_input=new_data.loc[:,['variance', 'skewness', 'curtosis', 'entropy']]\n",
    "new_result=new_data['class']\n",
    "#If ANN has 1 hidden layer we initialize it calling ANN_layer()\n",
    "\n",
    "# layers are initialized with parameter 1 :Number of inputs\n",
    "            #                parameter 2 :Number of neurons\n",
    "            #similarly hidden layer and output layer is initialised\n",
    "\n",
    "input11,x_test11, result11, y_test11 = train_test_split(new_input, new_result, test_size=0.35, random_state=42)\n",
    "Hlayer11=ANN_layers(4,128)\n",
    "#Hlayer12=ANN_layers(16,8)\n",
    "Output_layer1=ANN_layers(128,2)\n",
    "\n",
    "#Between each layer we have the activation function class with a \n",
    "#wide range of activation functions \n",
    "act11=activation()\n",
    "#act12=activation()\n",
    "\n",
    "# using the softmax function on the final layer\n",
    "loss_activation1=Activation_softmax_LCCE()\n",
    "\n",
    "#So for optimizer we can choose between Adam and SGD optimizer\n",
    "#Creating optimizer\n",
    "optimize1 = Adam_op(l_rate=0.05,decay=5e-7)\n",
    "\n",
    "#Loop to train in\n",
    "for epoch in range(4001):\n",
    "    \n",
    "    #pass input X through first layer\n",
    "    #Output of first layer passes through the firstactivation function\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    \n",
    "    Hlayer11.fpropogation(input11)\n",
    "    act11.fRelu(Hlayer11.output)\n",
    "    #Hlayer12.fpropogation(act11.output)\n",
    "    #act12.fRelu(Hlayer12.output)\n",
    "    Output_layer1.fpropogation(act11.output)\n",
    "    loss1 = loss_activation1.forward(Output_layer1.output,result11)\n",
    "    \n",
    "    #Argmax returns the predictions for the foward pass\n",
    "    prediction1 = np.argmax(loss_activation1.output,axis=1)\n",
    "    \n",
    "    #calculate values along first axis\n",
    "    if len(result11.shape)==2:\n",
    "        result11=np.argmax(result11,axis=1)\n",
    "    \n",
    "    \n",
    "    #returns the accuracy of our predictions in this pass\n",
    "    accuracy1= np.mean(prediction1==result11)\n",
    "    \n",
    "    #priniting for every 100Th epoch\n",
    "    if not epoch %100:\n",
    "        print(f'epoch:{epoch} ,'+\n",
    "              f'accuracy:{accuracy1:.3f} ,'+\n",
    "              f'loss:{loss1:.3f} ,'+\n",
    "              f'lr:{optimize.curr_l_rate},')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #back propogation \n",
    "    #The values are propogated backwards and the dinputs are updated\n",
    "    loss_activation1.backpass(loss_activation1.output,result11)\n",
    "    Output_layer1.Bpropogation(loss_activation1.dinputs)\n",
    "    act11.back(Output_layer1.dinputs)\n",
    "    #Hlayer12.Bpropogation(act12.dinputs)\n",
    "    #act11.back(Hlayer12.dinputs)\n",
    "    Hlayer11.Bpropogation(act11.dinputs)\n",
    "              \n",
    "    #Update in weights and the biases\n",
    "    optimize.initial_update_param()\n",
    "    optimize.update_params(Hlayer11)\n",
    "    #optimize.update_params(Hlayer12)\n",
    "    optimize.update_params(Output_layer1)\n",
    "    optimize.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x_test11\n",
    "y_test=y_test11\n",
    "Hlayer11.fpropogation(x_test)\n",
    "act11.fRelu(Hlayer11.output)\n",
    "#Hlayer12.fpropogation(act11.output)\n",
    "#act12.fRelu(Hlayer12.output)\n",
    "Output_layer1.fpropogation(act11.output)\n",
    "loss1 = loss_activation1.forward(Output_layer1.output,y_test)\n",
    "prediction1 = np.argmax(loss_activation1.output,axis=1)\n",
    "if len(y_test.shape)==2:\n",
    "    result11=np.argmax(y_test,axis=1)\n",
    "accuracy1= np.mean(prediction1==y_test)\n",
    "print(f'epoch:{epoch} ,'+f'accuracy:{accuracy1:.3f} ,'+\n",
    "       f'loss:{loss1:.3f} ,'+\n",
    "      f'lr:{optimize.curr_l_rate}')\n",
    "\n",
    "#predict values using model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.CM(y_test.tolist(),prediction1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
